{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D shape segmentation using TDA\n",
    "\n",
    "Our goal in this notebook is to do classification of points on 3D shapes, which is sometimes referred to as segmentation of 3D shapes. Intuitively, we want to give a label for each point of a 3D shape. For instance, if the shapes represent airplanes, the labels would be \"left wing\", \"right wing\", \"tail\" and so on. \n",
    "\n",
    "The idea of TDA is to represent each point of each shape with a persistence diagram, obtained with the sublevel sets of the geodesic distance to the point. More formally, if $x$ is a point on the 3D shape $S$, let $D(x) = \\text{Dgm}(f_x)$, where $f_x:y\\in S\\mapsto d_S(x,y)$ and $d_S(\\cdot,\\cdot)$ is the geodesic distance on $S$. Then $D(x)$ can be used as a powerful descriptor for $x$ which enjoys many desireable properties, such as stability and invariance to solid transformations of the shape. See [this article](https://diglib.eg.org/handle/10.1111/cgf12692) for more details.\n",
    "\n",
    "Let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this notebook requires [h5py](https://www.h5py.org/) (for reading the dataset of persistence diagrams), [pandas](https://pandas.pydata.org/) (for reading the labels), [sklearn_tda](https://github.com/MathieuCarriere/sklearn_tda) (for handling the persistence diagrams) and [scikit-learn](http://scikit-learn.org/stable/index.html) (for performing the final classification). It also makes use of numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I/O functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def diag_to_array(data):\n",
    "    dataset, num_diag = [], len(data[\"0\"].keys())\n",
    "    for dim in data.keys():\n",
    "        X = []\n",
    "        for diag in range(num_diag):\n",
    "            pers_diag = np.array(data[dim][str(diag)])\n",
    "            X.append(pers_diag)\n",
    "        dataset.append(X)\n",
    "    return dataset\n",
    "\n",
    "def diag_to_dict(D):\n",
    "    X = dict()\n",
    "    for f in D.keys():\n",
    "        df = diag_to_array(D[f])\n",
    "        for dim in range(len(df)):\n",
    "            X[str(dim) + \"_\" + f] = df[dim]\n",
    "    return X "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the dataset. \"train_diag.hdf5\" is a file containing the persistence diagrams. They were computed using [this code](https://github.com/MathieuCarriere/local-persistence-with-UF) on 3D shapes representing airplanes. The 3D shapes and their corresponding point labels (\"train.csv\") were retrieved from [this dataset](http://segeval.cs.princeton.edu/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "train_lab  = pd.read_csv(\"train.csv\")\n",
    "train_diag = diag_to_dict(h5py.File(\"train_diag.hdf5\", \"r\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separation of the dataset into train and test sets. The size of the test set is given by a percentage of the dataset size that you can specify by changing the test_size variable. Then the test set is obtained by randomly picking points in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Size of test set\n",
    "test_size = 0.4\n",
    "\n",
    "# Shuffle dataset and pick points for test set\n",
    "train_num_pts        = train_lab.shape[0]    \n",
    "perm                 = np.random.permutation(train_num_pts)\n",
    "limit                = np.int(test_size * train_num_pts)\n",
    "test_sub, train_sub  = perm[:limit], perm[limit:]\n",
    "\n",
    "# Create train and test labels with LabelEncoder from scikit-learn\n",
    "train_full_labels  = train_lab[\"part\"]\n",
    "le                 = LabelEncoder()\n",
    "train_labels       = np.array(le.fit_transform(train_full_labels[train_sub]))\n",
    "test_labels        = np.array(le.transform(train_full_labels[test_sub]))\n",
    "\n",
    "# Create train and test sets of persistence diagrams\n",
    "train_full_diag    = train_diag[\"1_geodesic\"]\n",
    "train_diag         = [train_full_diag[i] for i in train_sub]\n",
    "test_diag          = [train_full_diag[i] for i in test_sub]\n",
    "\n",
    "# Print sizes\n",
    "train_num_pts, test_num_pts = len(train_sub), len(test_sub)\n",
    "print(\"Number of train points = \" + str(train_num_pts))\n",
    "print(\"Number of test  points = \" + str(test_num_pts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a scikit-learn pipeline for processing the diagrams. The pipeline will:\n",
    "1. extract the points of the persistence diagrams with finite coordinates (i.e. the non essential points)\n",
    "2. rotate or not the diagrams (rotation is useful for persistence images)\n",
    "3. handle diagrams with vectorization or kernel methods using the sklearn_tda package\n",
    "4. train a classifier from the scikit-learn package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn_tda as tda\n",
    "from sklearn.pipeline        import Pipeline\n",
    "from sklearn.svm             import SVC\n",
    "from sklearn.ensemble        import RandomForestClassifier\n",
    "from sklearn.neighbors       import KNeighborsClassifier\n",
    "\n",
    "# Definition of pipeline\n",
    "pipe = Pipeline([(\"Separator\", tda.DiagramSelector(limit=np.inf, point_type=\"finite\")),\n",
    "                 (\"Rotator\",   tda.DiagramPreprocessor(scalers=[([0,1], tda.BirthPersistenceTransform())])),\n",
    "                 (\"TDA\",       tda.PersistenceImage()),\n",
    "                 (\"Estimator\", SVC())])\n",
    "\n",
    "# Parameters of pipeline. This is the place where you specify the methods you want to use to handle diagrams\n",
    "param =    [{\"Rotator__use\":        [False],\n",
    "             \"TDA\":                 [tda.SlicedWassersteinKernel()], \n",
    "             \"TDA__bandwidth\":      [0.1, 1.0],\n",
    "             \"TDA__num_directions\": [20],\n",
    "             \"Estimator\":           [SVC(kernel=\"precomputed\")]},\n",
    "            \n",
    "            {\"Rotator__use\":        [False],\n",
    "             \"TDA\":                 [tda.PersistenceWeightedGaussianKernel()], \n",
    "             \"TDA__bandwidth\":      [0.1, 1.0],\n",
    "             \"TDA__weight\":         [lambda x: np.arctan(x[1]-x[0])], \n",
    "             \"Estimator\":           [SVC(kernel=\"precomputed\")]},\n",
    "            \n",
    "            {\"Rotator__use\":        [True],\n",
    "             \"TDA\":                 [tda.PersistenceImage()], \n",
    "             \"TDA__resolution\":     [ [5,5], [6,6] ],\n",
    "             \"TDA__bandwidth\":      [0.01, 0.1, 1.0, 10.0],\n",
    "             \"Estimator\":           [SVC()]},\n",
    "            \n",
    "            {\"Rotator__use\":        [False],\n",
    "             \"TDA\":                 [tda.Landscape()], \n",
    "             \"TDA__resolution\":     [100],\n",
    "             \"Estimator\":           [RandomForestClassifier()]},\n",
    "           \n",
    "            {\"Rotator__use\":        [False],\n",
    "             \"TDA\":                 [tda.BottleneckDistance()], \n",
    "             \"TDA__wasserstein\":    [1],\n",
    "             \"TDA__delta\":          [0.1], \n",
    "             \"Estimator\":           [KNeighborsClassifier(metric=\"precomputed\")]}\n",
    "           ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final model is the best estimator found after 3-fold cross-validation of our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = GridSearchCV(pipe, param, cv=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is time to train the model. Since we perform cross-validation, the computation can be quite long, especially if using k-NN with Wasserstein distances, which is quite time-consuming. You may consider grabbing a cup of coffee at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.fit(train_diag, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training is finally over! Let us check what is the best method for persistence diagrams with respect to this classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we evaluate our model accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train accuracy = \" + str(model.score(train_diag, train_labels)))\n",
    "print(\"Test accuracy  = \" + str(model.score(test_diag,  test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the method you used, the accuracy can go up to ~90%, not bad! This score can actually get improved by adding other descriptors to the persistence diagrams (using for instance a FeatureUnion in the pipeline), but using TDA only already gives competitive accuracies!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
